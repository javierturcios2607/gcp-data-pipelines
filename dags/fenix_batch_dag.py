import pendulum
from airflow.models.dag import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor

# --- Â¡NUEVO! FUNCIÃ“N DE CALLBACK PARA ERRORES ---
# Airflow llamarÃ¡ a esta funciÃ³n si una tarea falla, pasÃ¡ndole un diccionario de contexto.
def on_task_failure(context):
    """
    Esta funciÃ³n se ejecuta cuando una tarea falla.
    Imprime un mensaje de alerta claro en los logs de Airflow.
    """
    task_instance = context['task_instance']
    print("="*50)
    print(f"ðŸš¨ Â¡ALERTA! La tarea ha fallado. ðŸš¨")
    print(f"   -> DAG: {task_instance.dag_id}")
    print(f"   -> Tarea: {task_instance.task_id}")
    print(f"   -> Fecha de EjecuciÃ³n: {task_instance.execution_date}")
    print(f"   -> Log URL: {task_instance.log_url}")
    print("="*50)

# --- DEFINICIÃ“N DE CONSTANTES ---
PROJECT_ID = "riverajavier-dev"
DATASET_ID = "fenix_dataset"
GCS_BUCKET = "fenix-data-lake-dev-riverajavier-dev"

EXECUTION_DATE_MACRO = "{{ ds }}"

LOAD_OPTIMIZED_TABLE_SQL = f"""
    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.sales_optimized`
    PARTITION BY sale_date
    CLUSTER BY product_id
    AS
    SELECT *
    FROM `{PROJECT_ID}.{DATASET_ID}.sales_raw_external`
    WHERE sale_date = @execution_date;
"""

# --- DEFINICIÃ“N DEL DAG ---
with DAG(
    dag_id="fenix_daily_batch_load",
    start_date=pendulum.datetime(2025, 10, 30, tz="UTC"),
    schedule_interval="0 3 * * *",
    catchup=False,
    # Â¡NUEVO! Podemos definir callbacks a nivel de DAG, que se aplicarÃ¡n a todas las tareas.
    on_failure_callback=on_task_failure,
    tags=["fenix_project", "batch", "data_engineering"],
) as dag:

    # --- TAREA #1: EL SENSOR (EL GUARDIA) ---
    wait_for_sales_file = GCSObjectExistenceSensor(
        task_id="wait_for_sales_file",
        bucket=GCS_BUCKET,
        object=f"sales/raw/dt={EXECUTION_DATE_MACRO}/_SUCCESS",
        mode="poke",
        poke_interval=60, # Reducido a 60s para pruebas mÃ¡s rÃ¡pidas
        timeout=60 * 30, # Timeout de 30 mins
    )

    # --- TAREA #2: LA CARGA A BIGQUERY (EL MÃšSICO) ---
    load_to_optimized_table = BigQueryInsertJobOperator(
        task_id="load_sales_optimized_from_external",
        configuration={
            "query": {
                "query": LOAD_OPTIMIZED_TABLE_SQL,
                "useLegacySql": False,
                "queryParameters": [
                    {
                        "name": "execution_date",
                        "parameterType": {"type": "DATE"},
                        "parameterValue": {"value": EXECUTION_DATE_MACRO},
                    }
                ],
            }
        },
        location="US",
    )

    # --- DEFINICIÃ“N DE DEPENDENCIAS ---
    wait_for_sales_file >> load_to_optimized_table
